{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image to Monet Cycle GAN\n",
    "\n",
    "This project aims to solve this [Kaggle Contest](https://www.kaggle.com/competitions/gan-getting-started/overview). I based my code in the following [Cycle GAN Implementation Guide](https://www.kaggle.com/code/ttymonkey/cyclegan-starter).\n",
    "\n",
    "## Objective\n",
    "\n",
    "The main objective of this contest is to generate Monet images from normal images. Submit your generated images and be top target in the leaderboard.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "As the contest ask you the submit format needs to be a notebook, I am using the GPU P100 architecture.\n",
    "\n",
    "## Initial imports\n",
    "\n",
    "I will need some libraries for data processing and PyTorch for easy to use training a Cycle GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T11:54:40.019898Z",
     "iopub.status.busy": "2024-04-28T11:54:40.019307Z",
     "iopub.status.idle": "2024-04-28T11:54:44.610319Z",
     "shell.execute_reply": "2024-04-28T11:54:44.609481Z",
     "shell.execute_reply.started": "2024-04-28T11:54:40.019858Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used Folders listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T11:54:44.612720Z",
     "iopub.status.busy": "2024-04-28T11:54:44.612318Z",
     "iopub.status.idle": "2024-04-28T11:54:44.620887Z",
     "shell.execute_reply": "2024-04-28T11:54:44.620023Z",
     "shell.execute_reply.started": "2024-04-28T11:54:44.612695Z"
    }
   },
   "outputs": [],
   "source": [
    "root_path = \"/kaggle/input/gan-getting-started\"\n",
    "os.listdir(root_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "First, I want to see what type of image I am dealing with, the size and ways of how to read it and processes it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T11:54:44.622301Z",
     "iopub.status.busy": "2024-04-28T11:54:44.621975Z",
     "iopub.status.idle": "2024-04-28T11:54:44.633893Z",
     "shell.execute_reply": "2024-04-28T11:54:44.633168Z",
     "shell.execute_reply.started": "2024-04-28T11:54:44.622278Z"
    }
   },
   "outputs": [],
   "source": [
    "read_img = lambda path: cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T11:54:44.635871Z",
     "iopub.status.busy": "2024-04-28T11:54:44.635245Z",
     "iopub.status.idle": "2024-04-28T11:54:45.271755Z",
     "shell.execute_reply": "2024-04-28T11:54:45.270841Z",
     "shell.execute_reply.started": "2024-04-28T11:54:44.635840Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = f\"{root_path}/photo_jpg\"\n",
    "sample_photo = read_img(os.path.join(data_path, os.listdir(data_path)[0]))\n",
    "\n",
    "data_path = f\"{root_path}/monet_jpg\"\n",
    "sample_monet = read_img(os.path.join(data_path, os.listdir(data_path)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T11:54:45.275337Z",
     "iopub.status.busy": "2024-04-28T11:54:45.274684Z",
     "iopub.status.idle": "2024-04-28T11:54:45.281148Z",
     "shell.execute_reply": "2024-04-28T11:54:45.280149Z",
     "shell.execute_reply.started": "2024-04-28T11:54:45.275298Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_photo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T11:54:45.282680Z",
     "iopub.status.busy": "2024-04-28T11:54:45.282385Z",
     "iopub.status.idle": "2024-04-28T11:54:45.293155Z",
     "shell.execute_reply": "2024-04-28T11:54:45.292003Z",
     "shell.execute_reply.started": "2024-04-28T11:54:45.282656Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_photo.min(), sample_photo.max(), sample_photo.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T11:54:45.295132Z",
     "iopub.status.busy": "2024-04-28T11:54:45.294511Z",
     "iopub.status.idle": "2024-04-28T11:54:45.852694Z",
     "shell.execute_reply": "2024-04-28T11:54:45.851731Z",
     "shell.execute_reply.started": "2024-04-28T11:54:45.295098Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "plt.title(\"Photo\")\n",
    "plt.imshow(sample_photo)  \n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Photo\")\n",
    "plt.imshow(sample_monet)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cycle GAN Implementation\n",
    "\n",
    "A cycle GAN is similar to the traditional GAN architecture, but it do not need equivalent pairs of images to learn the context of the desired output. Such as the example above shows, it not exist equivalent pairs in this dataset.\n",
    "\n",
    "### Initialization\n",
    "\n",
    "The size of the images are 256x256 images with 3 channels, I will save it in the following global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T11:54:45.854233Z",
     "iopub.status.busy": "2024-04-28T11:54:45.853852Z",
     "iopub.status.idle": "2024-04-28T11:54:45.858901Z",
     "shell.execute_reply": "2024-04-28T11:54:45.857986Z",
     "shell.execute_reply.started": "2024-04-28T11:54:45.854198Z"
    }
   },
   "outputs": [],
   "source": [
    "IMG_H, IMG_W, IMG_C = 256, 256, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then starting the implementation...\n",
    "\n",
    "### Downsampling\n",
    "\n",
    "The first part of a traditional Cycle GAN architecture is the downsampling, it reduces the resolution of the images giving the network the most relevant information of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T11:54:45.860723Z",
     "iopub.status.busy": "2024-04-28T11:54:45.860442Z",
     "iopub.status.idle": "2024-04-28T11:54:45.870333Z",
     "shell.execute_reply": "2024-04-28T11:54:45.869479Z",
     "shell.execute_reply.started": "2024-04-28T11:54:45.860699Z"
    }
   },
   "outputs": [],
   "source": [
    "def downsample(in_channels, out_channels, kernel_size, norm=True):\n",
    "    downsample_block = [nn.Conv2d(in_channels=in_channels,\n",
    "                                  out_channels=out_channels,\n",
    "                                  kernel_size=kernel_size,\n",
    "                                  stride=2,\n",
    "                                  padding=(kernel_size - 1) // 2,\n",
    "                                  bias=False)\n",
    "                       ]\n",
    "    if norm:\n",
    "        downsample_block.append(nn.GroupNorm(num_groups=out_channels,\n",
    "                                             num_channels=out_channels))\n",
    "    \n",
    "    downsample_block.append(nn.LeakyReLU())\n",
    "    return nn.Sequential(*downsample_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T11:54:45.871587Z",
     "iopub.status.busy": "2024-04-28T11:54:45.871324Z",
     "iopub.status.idle": "2024-04-28T11:54:45.976529Z",
     "shell.execute_reply": "2024-04-28T11:54:45.975710Z",
     "shell.execute_reply.started": "2024-04-28T11:54:45.871564Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.randn(1,IMG_C, IMG_H, IMG_W)\n",
    "\n",
    "out = downsample(3, 3, 3)(x)\n",
    "\n",
    "assert IMG_C == out.shape[1] \n",
    "assert IMG_H == out.shape[2] * 2\n",
    "assert IMG_W == out.shape[3] * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling \n",
    "You can define the upsamplig step as the oposite of upsampling. From information, it upscales the image genereting a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T11:54:45.978112Z",
     "iopub.status.busy": "2024-04-28T11:54:45.977763Z",
     "iopub.status.idle": "2024-04-28T11:54:45.985632Z",
     "shell.execute_reply": "2024-04-28T11:54:45.984745Z",
     "shell.execute_reply.started": "2024-04-28T11:54:45.978082Z"
    }
   },
   "outputs": [],
   "source": [
    "def upsample(in_channels, out_channels, kernel_size, dropout=False):\n",
    "    downsample_block = [nn.ConvTranspose2d(in_channels=in_channels,\n",
    "                                           out_channels=out_channels,\n",
    "                                           kernel_size=kernel_size,\n",
    "                                           stride=2,\n",
    "                                           padding=(kernel_size - 1) // 2,\n",
    "                                           bias=False),\n",
    "                        nn.GroupNorm(num_groups=out_channels,\n",
    "                                     num_channels=out_channels)\n",
    "                       ]\n",
    "    if dropout:\n",
    "        downsample_block.append(nn.Dropout(0.5))\n",
    "    \n",
    "    downsample_block.append(nn.ReLU())\n",
    "    return nn.Sequential(*downsample_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T11:54:45.987425Z",
     "iopub.status.busy": "2024-04-28T11:54:45.986989Z",
     "iopub.status.idle": "2024-04-28T11:54:46.039835Z",
     "shell.execute_reply": "2024-04-28T11:54:46.038776Z",
     "shell.execute_reply.started": "2024-04-28T11:54:45.987401Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.randn(1,IMG_C, IMG_H, IMG_W)\n",
    "\n",
    "out = upsample(3, 3, 4)(x)\n",
    "\n",
    "assert IMG_C == out.shape[1] \n",
    "assert IMG_H == out.shape[2] // 2\n",
    "assert IMG_W == out.shape[3] // 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, it is needed to initialize the network convolutional layers with random values, the following cell do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T11:54:46.041794Z",
     "iopub.status.busy": "2024-04-28T11:54:46.041470Z",
     "iopub.status.idle": "2024-04-28T11:54:46.049627Z",
     "shell.execute_reply": "2024-04-28T11:54:46.048596Z",
     "shell.execute_reply.started": "2024-04-28T11:54:46.041768Z"
    }
   },
   "outputs": [],
   "source": [
    "def weight_init(m):\n",
    "    if any(isinstance(m, _m) for _m in [nn.Conv2d, nn.ConvTranspose2d]):\n",
    "        nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator\n",
    "\n",
    "It is the part of the layer that generates new images from the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T11:54:46.055243Z",
     "iopub.status.busy": "2024-04-28T11:54:46.054972Z",
     "iopub.status.idle": "2024-04-28T11:54:46.068600Z",
     "shell.execute_reply": "2024-04-28T11:54:46.067820Z",
     "shell.execute_reply.started": "2024-04-28T11:54:46.055221Z"
    }
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = self._init_encoder()\n",
    "        self.decoder = self._init_decoder()\n",
    "        \n",
    "        self.out = nn.ConvTranspose2d(in_channels=128,\n",
    "                                      out_channels=3,\n",
    "                                      kernel_size=4,\n",
    "                                      stride=2, padding=1)\n",
    "        self.act = nn.Tanh()\n",
    "        \n",
    "        self.apply(weight_init)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        skips = [] \n",
    "        for layer in self.encoder:\n",
    "            x = layer(x)\n",
    "            skips.append(x)\n",
    "        \n",
    "        skips = reversed(skips[:-1])\n",
    "        \n",
    "        for layer, skip in zip(self.decoder, skips):\n",
    "            x = layer(x)\n",
    "            x = torch.cat((x, skip), dim=1)\n",
    "                \n",
    "        out = self.act(self.out(x))\n",
    "        \n",
    "        return out\n",
    "        \n",
    "        \n",
    "    def _init_encoder(self):\n",
    "        kernel_size = 4\n",
    "        in_channels = 64\n",
    "        \n",
    "        out_channels = [128, 256, 512, 512, 512, 512]\n",
    "        \n",
    "        encoder = [downsample(in_channels=3, out_channels=in_channels, \n",
    "                              kernel_size=kernel_size, norm=False)]\n",
    "        \n",
    "        for out_ch in out_channels:\n",
    "            encoder.append(downsample(in_channels=in_channels, out_channels=out_ch,\n",
    "                                      kernel_size=kernel_size))\n",
    "            in_channels = out_ch\n",
    "        \n",
    "        return nn.Sequential(*encoder)\n",
    "    \n",
    "    def _init_decoder(self):\n",
    "        base_channels = 512\n",
    "        kernel_size = 4\n",
    "        in_channels = [512, 1024, 1024, 1024, 512, 256]\n",
    "        \n",
    "        out_channels  = [512, 512, 512, 256, 128, 64]\n",
    "        dropout = [True, True, True, False, False, False]\n",
    "        \n",
    "        decoder = [upsample(in_channels=in_ch, out_channels=out_ch, \n",
    "                            kernel_size=kernel_size, dropout=drop) \n",
    "                   \n",
    "                   for in_ch, out_ch, drop in zip(in_channels, out_channels, dropout)]\n",
    "        \n",
    "        \n",
    "        return nn.Sequential(*decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T11:54:46.069815Z",
     "iopub.status.busy": "2024-04-28T11:54:46.069580Z",
     "iopub.status.idle": "2024-04-28T11:54:47.112480Z",
     "shell.execute_reply": "2024-04-28T11:54:47.111513Z",
     "shell.execute_reply.started": "2024-04-28T11:54:46.069795Z"
    }
   },
   "outputs": [],
   "source": [
    "g = Generator()\n",
    "x = torch.randn(1,3,256,256)\n",
    "\n",
    "enc_out = g.encoder(x)\n",
    "gen_out = g(x)\n",
    "\n",
    "assert enc_out.shape[1] == 512\n",
    "assert enc_out.shape[2] == x.shape[2] // 128\n",
    "assert enc_out.shape[3] == x.shape[3] // 128\n",
    "\n",
    "assert gen_out.shape[1] == x.shape[1]\n",
    "assert gen_out.shape[2] == x.shape[2]\n",
    "assert gen_out.shape[3] == x.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T11:54:47.114325Z",
     "iopub.status.busy": "2024-04-28T11:54:47.113838Z",
     "iopub.status.idle": "2024-04-28T11:54:47.221496Z",
     "shell.execute_reply": "2024-04-28T11:54:47.220404Z",
     "shell.execute_reply.started": "2024-04-28T11:54:47.114291Z"
    }
   },
   "outputs": [],
   "source": [
    "convolutions = []\n",
    "for _, m in g.named_modules():\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        convolutions.append(m.weight.flatten().clone().detach().numpy())\n",
    "\n",
    "convolutions = np.concatenate(convolutions)\n",
    "convolutions.mean(), convolutions.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "\n",
    "It is the part of the network which difference fake images from the real ones. In a cycle GAN it is necesary because we need to calculate the loss in some way and the discriminator will gives us this value from its evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T11:54:47.222861Z",
     "iopub.status.busy": "2024-04-28T11:54:47.222579Z",
     "iopub.status.idle": "2024-04-28T11:54:47.231935Z",
     "shell.execute_reply": "2024-04-28T11:54:47.231018Z",
     "shell.execute_reply.started": "2024-04-28T11:54:47.222839Z"
    }
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        base_channels = 64\n",
    "        kernel_size = 4\n",
    "        self.discriminator = nn.Sequential(\n",
    "            downsample(in_channels=3, out_channels=base_channels, \n",
    "                       kernel_size=kernel_size, norm=False),\n",
    "            downsample(in_channels=base_channels, out_channels=base_channels * 2, \n",
    "                       kernel_size=kernel_size),\n",
    "            downsample(in_channels=base_channels * 2, out_channels=base_channels * 4, \n",
    "                       kernel_size=kernel_size),\n",
    "            nn.ZeroPad2d(padding=(0, 2, 0, 2)),\n",
    "            nn.Conv2d(in_channels=base_channels * 4, out_channels=base_channels * 8,\n",
    "                      kernel_size=4, stride=1, bias=False),\n",
    "            nn.GroupNorm(num_groups=base_channels * 8,\n",
    "                         num_channels=base_channels * 8),\n",
    "            nn.ZeroPad2d(padding=(0, 2, 0, 2)),      \n",
    "            nn.Conv2d(in_channels=base_channels * 8, out_channels=1,\n",
    "                      kernel_size=4, stride=1),\n",
    "        )\n",
    "        \n",
    "        self.apply(weight_init)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.discriminator(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T11:54:47.233894Z",
     "iopub.status.busy": "2024-04-28T11:54:47.233362Z",
     "iopub.status.idle": "2024-04-28T11:54:47.342002Z",
     "shell.execute_reply": "2024-04-28T11:54:47.340975Z",
     "shell.execute_reply.started": "2024-04-28T11:54:47.233857Z"
    }
   },
   "outputs": [],
   "source": [
    "d = Discriminator()\n",
    "x = torch.randn(1,3,256,256)\n",
    "\n",
    "assert d(x).shape[1] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T11:54:47.343629Z",
     "iopub.status.busy": "2024-04-28T11:54:47.343272Z",
     "iopub.status.idle": "2024-04-28T11:54:47.354303Z",
     "shell.execute_reply": "2024-04-28T11:54:47.353286Z",
     "shell.execute_reply.started": "2024-04-28T11:54:47.343592Z"
    }
   },
   "outputs": [],
   "source": [
    "torch_sample = torch.tensor(sample_photo, dtype=torch.float32)\n",
    "\n",
    "torch_sample = (torch_sample / 255.0 - 0.5) / 0.5\n",
    "\n",
    "torch_sample = torch_sample.permute(2,0,1)\n",
    "\n",
    "torch_sample = torch_sample.unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the generator\n",
    "\n",
    "From initial state, the generator will give some information about the Original Photo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T11:54:47.356172Z",
     "iopub.status.busy": "2024-04-28T11:54:47.355676Z",
     "iopub.status.idle": "2024-04-28T11:54:48.138613Z",
     "shell.execute_reply": "2024-04-28T11:54:48.137785Z",
     "shell.execute_reply.started": "2024-04-28T11:54:47.356136Z"
    }
   },
   "outputs": [],
   "source": [
    "monet_generator = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T11:54:48.140189Z",
     "iopub.status.busy": "2024-04-28T11:54:48.139821Z",
     "iopub.status.idle": "2024-04-28T11:54:48.291534Z",
     "shell.execute_reply": "2024-04-28T11:54:48.290437Z",
     "shell.execute_reply.started": "2024-04-28T11:54:48.140157Z"
    }
   },
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    monet_generator.eval()\n",
    "    to_monet = monet_generator(torch_sample).detach()\n",
    "    to_monet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T11:54:48.294014Z",
     "iopub.status.busy": "2024-04-28T11:54:48.292701Z",
     "iopub.status.idle": "2024-04-28T11:54:48.817009Z",
     "shell.execute_reply": "2024-04-28T11:54:48.816008Z",
     "shell.execute_reply.started": "2024-04-28T11:54:48.293982Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Original Photo\")\n",
    "plt.imshow(torch_sample[0].permute(1,2,0) * 0.5 + 0.5)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Monet Photo\")\n",
    "plt.imshow(to_monet[0].permute(1,2,0) * 0.5 + 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cycle GAN implementation\n",
    "\n",
    "As I mention in the introduction of this section, a Cycle GAN distinguish the field of the desired target looping from one field to another. The basic logic of a Cycle GAN is two GANs competing between each other.\n",
    "\n",
    "One trying to translate to field (Original -> Monet) and the other with (Monet -> Original.\n",
    "\n",
    "In the training, both GANs generators are constantly trying to mislead their rivals.\n",
    "\n",
    "Finally, it is needed to calculate the loss between each type of generated image: fake and original, from every field: normal and Monet. This will give the network a way to generate better images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T11:54:48.818635Z",
     "iopub.status.busy": "2024-04-28T11:54:48.818355Z",
     "iopub.status.idle": "2024-04-28T11:54:48.836386Z",
     "shell.execute_reply": "2024-04-28T11:54:48.835430Z",
     "shell.execute_reply.started": "2024-04-28T11:54:48.818610Z"
    }
   },
   "outputs": [],
   "source": [
    "class CycleGAN(nn.Module):\n",
    "    \n",
    "    def __init__(self, lr=2e-4, lambda_cycle=10):  # Not sure what that is yet\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gen_monet = Generator()\n",
    "        self.optim_gen_monet = optim.Adam(self.gen_monet.parameters(),\n",
    "                                          lr=lr,\n",
    "                                          betas=(0.5, 0.999))\n",
    "        self.gen_photo = Generator()\n",
    "        self.optim_gen_photo = optim.Adam(self.gen_photo.parameters(),\n",
    "                                          lr=lr,\n",
    "                                          betas=(0.5, 0.999))\n",
    "        \n",
    "        self.disc_monet = Discriminator()\n",
    "        self.optim_disc_monet = optim.Adam(self.disc_monet.parameters(),\n",
    "                                          lr=lr,\n",
    "                                          betas=(0.5, 0.999))\n",
    "        self.disc_photo = Discriminator()\n",
    "        self.optim_disc_photo = optim.Adam(self.disc_photo.parameters(),\n",
    "                                          lr=lr,\n",
    "                                          betas=(0.5, 0.999))\n",
    "        \n",
    "        self.loss_l2 = nn.MSELoss()\n",
    "        self.loss_l1 = nn.L1Loss()\n",
    "        \n",
    "        self.lambda_cycle = lambda_cycle\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.gen_monet(x)\n",
    "    \n",
    "    def forward_step(self, x):\n",
    "        x_monet, x_photo = x\n",
    "        \n",
    "        self.optim_zero_grad()\n",
    "        \n",
    "        x_fake_monet = self.gen_monet(x_photo)\n",
    "        x_fake_photo = self.gen_photo(x_monet)\n",
    "        \n",
    "        d_real_monet = self.disc_monet(x_monet)\n",
    "        d_fake_monet = self.disc_monet(x_fake_monet.detach())\n",
    "        \n",
    "        d_real_photo = self.disc_photo(x_photo)\n",
    "        d_fake_photo = self.disc_photo(x_fake_photo.detach())\n",
    "        \n",
    "        d_loss_monet = self.disc_loss(x_real=d_real_monet,\n",
    "                                      x_fake=d_fake_monet)\n",
    "        \n",
    "        d_loss_photo = self.disc_loss(x_real=d_real_photo,\n",
    "                                      x_fake=d_fake_photo)\n",
    "        \n",
    "        d_loss = d_loss_monet + d_loss_photo\n",
    "        \n",
    "        d_loss.backward()\n",
    "        \n",
    "        self.optim_disc_monet.step()\n",
    "        self.optim_disc_photo.step()\n",
    "        \n",
    "        x_adv_monet = self.disc_monet(x_fake_monet)\n",
    "        x_adv_photo = self.disc_photo(x_fake_photo)  \n",
    "        \n",
    "        x_cycled_monet = self.gen_monet(x_fake_photo)\n",
    "        x_cycled_photo = self.gen_photo(x_fake_monet)\n",
    "        \n",
    "        g_loss_adv_monet = self.loss_l2(torch.ones_like(x_adv_monet), x_adv_monet)\n",
    "        g_loss_adv_photo = self.loss_l2(torch.ones_like(x_adv_photo), x_adv_photo)\n",
    "        \n",
    "        g_loss_cycle_monet = self.loss_l1(x_monet, x_cycled_monet) * self.lambda_cycle\n",
    "        g_loss_cycle_photo = self.loss_l1(x_photo, x_cycled_photo) * self.lambda_cycle\n",
    "        \n",
    "        g_loss_monet = g_loss_adv_monet + g_loss_cycle_monet \n",
    "        g_loss_photo = g_loss_adv_photo + g_loss_cycle_photo \n",
    "        g_loss = g_loss_monet + g_loss_photo\n",
    "        \n",
    "        g_loss.backward()\n",
    "        \n",
    "        self.optim_gen_monet.step()\n",
    "        self.optim_gen_photo.step()\n",
    "        \n",
    "        return {\n",
    "            \"g_monet\" : g_loss_monet,\n",
    "            \"g_photo\" : g_loss_photo,\n",
    "            \"d_monet\" : d_loss_monet,\n",
    "            \"d_photo\" : d_loss_photo\n",
    "        }\n",
    "        \n",
    "    def disc_loss(self, x_real, x_fake):\n",
    "        loss_real = self.loss_l2(torch.ones_like(x_real), x_real)\n",
    "        loss_fake = self.loss_l2(torch.zeros_like(x_fake), x_fake)\n",
    "        \n",
    "        return (loss_real + loss_fake) * 0.5\n",
    "    \n",
    "    def optim_zero_grad(self):\n",
    "        self.optim_gen_monet.zero_grad()\n",
    "        self.optim_gen_photo.zero_grad()\n",
    "        \n",
    "        self.optim_disc_monet.zero_grad()\n",
    "        self.optim_disc_photo.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T11:54:48.837778Z",
     "iopub.status.busy": "2024-04-28T11:54:48.837494Z",
     "iopub.status.idle": "2024-04-28T11:54:52.762271Z",
     "shell.execute_reply": "2024-04-28T11:54:52.761366Z",
     "shell.execute_reply.started": "2024-04-28T11:54:48.837755Z"
    }
   },
   "outputs": [],
   "source": [
    "cycle_gan = CycleGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T11:54:52.763989Z",
     "iopub.status.busy": "2024-04-28T11:54:52.763459Z",
     "iopub.status.idle": "2024-04-28T11:54:56.611803Z",
     "shell.execute_reply": "2024-04-28T11:54:56.610766Z",
     "shell.execute_reply.started": "2024-04-28T11:54:52.763962Z"
    }
   },
   "outputs": [],
   "source": [
    "x_real = torch.ones(1, 3, 256, 256)\n",
    "x_fake = torch.zeros_like(x_real)\n",
    "cycle_gan.forward_step((x_real, x_fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T11:54:56.613854Z",
     "iopub.status.busy": "2024-04-28T11:54:56.613162Z",
     "iopub.status.idle": "2024-04-28T11:54:56.620698Z",
     "shell.execute_reply": "2024-04-28T11:54:56.619935Z",
     "shell.execute_reply.started": "2024-04-28T11:54:56.613819Z"
    }
   },
   "outputs": [],
   "source": [
    "to_tensor = lambda x: torch.tensor(x, dtype=torch.float32).permute(2,0,1).unsqueeze(0) / 127.5 - 1.0\n",
    "from_tensor = lambda x: ((x.squeeze().permute(1,2,0).numpy() + 1.0) * 127.5).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T11:54:56.622039Z",
     "iopub.status.busy": "2024-04-28T11:54:56.621769Z",
     "iopub.status.idle": "2024-04-28T11:54:56.629773Z",
     "shell.execute_reply": "2024-04-28T11:54:56.628985Z",
     "shell.execute_reply.started": "2024-04-28T11:54:56.622017Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T11:54:56.631126Z",
     "iopub.status.busy": "2024-04-28T11:54:56.630779Z",
     "iopub.status.idle": "2024-04-28T11:54:57.009808Z",
     "shell.execute_reply": "2024-04-28T11:54:57.008832Z",
     "shell.execute_reply.started": "2024-04-28T11:54:56.631097Z"
    }
   },
   "outputs": [],
   "source": [
    "tensor_monet = to_tensor(sample_monet).to(device)\n",
    "tensor_photo = to_tensor(sample_photo).to(device)\n",
    "\n",
    "tensor_monet.min(), tensor_monet.max(), tensor_monet.shape, tensor_monet.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading\n",
    "\n",
    "Then I will load the full photos dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T11:54:57.011427Z",
     "iopub.status.busy": "2024-04-28T11:54:57.011072Z",
     "iopub.status.idle": "2024-04-28T11:54:57.020322Z",
     "shell.execute_reply": "2024-04-28T11:54:57.019524Z",
     "shell.execute_reply.started": "2024-04-28T11:54:57.011396Z"
    }
   },
   "outputs": [],
   "source": [
    "class MonetDataset(Dataset):\n",
    "    def __init__(self, root_path):\n",
    "        \n",
    "        self.root_path = root_path\n",
    "        self.photo_paths = os.listdir(f\"{root_path}/photo_jpg\")\n",
    "        self.monet_paths = os.listdir(f\"{root_path}/monet_jpg\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.photo_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path_photo = f\"{root_path}/photo_jpg/{self.photo_paths[idx]}\"\n",
    "        \n",
    "        path_monet = f\"{root_path}/monet_jpg/{random.choices(self.monet_paths, k=1)[0]}\"\n",
    "        \n",
    "        x_photo = self.to_tensor(self.read_img(path_photo))\n",
    "        x_monet = self.to_tensor(self.read_img(path_monet))\n",
    "        \n",
    "        return x_monet, x_photo\n",
    "        \n",
    "    def to_tensor(self, x):\n",
    "        return torch.tensor(x, dtype=torch.float32).permute(2,0,1) / 127.5 - 1.0\n",
    "        \n",
    "    def read_img(self, path):\n",
    "        return cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "I will set the number of epochs to 50 and the batch size to 4. Originally the number of epochs was 30 and the batch 32. The main reason to this change is the closeness to desired performance and the 4GB max memory which, during training, it fills 2.4 GB of the total graphic memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T11:54:57.021709Z",
     "iopub.status.busy": "2024-04-28T11:54:57.021402Z",
     "iopub.status.idle": "2024-04-28T11:54:57.030657Z",
     "shell.execute_reply": "2024-04-28T11:54:57.029921Z",
     "shell.execute_reply.started": "2024-04-28T11:54:57.021686Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 35\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "loss_epoch = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T11:54:57.031926Z",
     "iopub.status.busy": "2024-04-28T11:54:57.031672Z",
     "iopub.status.idle": "2024-04-28T11:54:58.924216Z",
     "shell.execute_reply": "2024-04-28T11:54:58.923242Z",
     "shell.execute_reply.started": "2024-04-28T11:54:57.031904Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = MonetDataset(root_path=root_path)\n",
    "data_loader = DataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=4)\n",
    "\n",
    "cycle_gan = CycleGAN(lr=1e-6).to(device)\n",
    "loss_epoch = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T11:54:58.925810Z",
     "iopub.status.busy": "2024-04-28T11:54:58.925461Z",
     "iopub.status.idle": "2024-04-28T15:01:06.171656Z",
     "shell.execute_reply": "2024-04-28T15:01:06.170598Z",
     "shell.execute_reply.started": "2024-04-28T11:54:58.925777Z"
    }
   },
   "outputs": [],
   "source": [
    "VALIDATE_EVERY = 0.1 * NUM_EPOCHS\n",
    "PHOTO_TO_MONET = []\n",
    "\n",
    "for i in range(1, NUM_EPOCHS + 1):  # Starting at one for tqdm\n",
    "    \n",
    "    with tqdm(total=len(data_loader),\n",
    "          desc=f\"Epoch {i}/{NUM_EPOCHS}\",\n",
    "          ascii=True, \n",
    "          colour=\"green\") as pbar:\n",
    "        \n",
    "        loss_batch = defaultdict(list)\n",
    "        for x in data_loader:\n",
    "            x_monet = x[0].to(device)\n",
    "            x_photo = x[1].to(device)\n",
    "\n",
    "            loss = cycle_gan.forward_step((x_monet, x_photo))\n",
    "\n",
    "            for k, v in loss.items():\n",
    "                loss_batch[k].append(v.item())\n",
    "\n",
    "            pbar.set_postfix({k:v.item() for k, v in loss.items()})\n",
    "            pbar.update()\n",
    "            \n",
    "        for k, v in loss_batch.items():\n",
    "            loss_epoch[k].append(np.mean(v))\n",
    "        \n",
    "        if i % VALIDATE_EVERY == 0:\n",
    "            with torch.inference_mode(mode=True):\n",
    "                cycle_gan.eval()\n",
    "                monet_pred = cycle_gan(tensor_photo)\n",
    "            \n",
    "            PHOTO_TO_MONET.append(monet_pred)\n",
    "            cycle_gan.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Evaluation\n",
    "\n",
    "In the following figure you will see the Generated images from both GANS and the real ones loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T15:01:06.173429Z",
     "iopub.status.busy": "2024-04-28T15:01:06.173124Z",
     "iopub.status.idle": "2024-04-28T15:01:07.179909Z",
     "shell.execute_reply": "2024-04-28T15:01:07.179003Z",
     "shell.execute_reply.started": "2024-04-28T15:01:06.173401Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.plot(range(len(loss_epoch[\"g_monet\"])), loss_epoch[\"g_monet\"])\n",
    "plt.title(\"G Monet\")\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(range(len(loss_epoch[\"g_photo\"])), loss_epoch[\"g_photo\"])\n",
    "plt.title(\"G Photo\")\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(range(len(loss_epoch[\"d_monet\"])), loss_epoch[\"d_monet\"])\n",
    "plt.title(\"D Monet\")\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.plot(range(len(loss_epoch[\"d_photo\"])), loss_epoch[\"d_photo\"])\n",
    "plt.title(\"D Photo\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T15:01:07.181720Z",
     "iopub.status.busy": "2024-04-28T15:01:07.181345Z",
     "iopub.status.idle": "2024-04-28T15:01:07.904836Z",
     "shell.execute_reply": "2024-04-28T15:01:07.903806Z",
     "shell.execute_reply.started": "2024-04-28T15:01:07.181686Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Photo\")\n",
    "plt.imshow(from_tensor(tensor_photo.cpu()))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Monet\")\n",
    "plt.imshow(sample_monet)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T15:01:07.906503Z",
     "iopub.status.busy": "2024-04-28T15:01:07.906213Z",
     "iopub.status.idle": "2024-04-28T15:01:09.527631Z",
     "shell.execute_reply": "2024-04-28T15:01:09.525663Z",
     "shell.execute_reply.started": "2024-04-28T15:01:07.906478Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 5, figsize=(20, 10))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, img in enumerate(PHOTO_TO_MONET):\n",
    "    axes[i].imshow(from_tensor(img.cpu()))\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f\"Photo to Monet at: {(i+1) * 10}%\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T15:01:09.530315Z",
     "iopub.status.busy": "2024-04-28T15:01:09.529784Z",
     "iopub.status.idle": "2024-04-28T15:01:10.588031Z",
     "shell.execute_reply": "2024-04-28T15:01:10.586839Z",
     "shell.execute_reply.started": "2024-04-28T15:01:09.530268Z"
    }
   },
   "outputs": [],
   "source": [
    "import PIL\n",
    "! mkdir ../images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T15:01:10.590004Z",
     "iopub.status.busy": "2024-04-28T15:01:10.589632Z",
     "iopub.status.idle": "2024-04-28T15:02:47.642274Z",
     "shell.execute_reply": "2024-04-28T15:02:47.641310Z",
     "shell.execute_reply.started": "2024-04-28T15:01:10.589965Z"
    }
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "data_loader = DataLoader(dataset=dataset, batch_size=1)\n",
    "with torch.inference_mode(mode=True):\n",
    "    cycle_gan.eval()\n",
    "    \n",
    "    for x in data_loader:\n",
    "        photo, _ = x\n",
    "        \n",
    "        pred = cycle_gan(photo.to(device)).cpu().squeeze().numpy()\n",
    "        pred = np.transpose(pred, (1,2,0))\n",
    "        \n",
    "        pred = (pred * 127.5 + 127.5).astype(np.uint8)\n",
    "        \n",
    "        pred_img = PIL.Image.fromarray(pred)\n",
    "        pred_img.save(\"../images/\" + str(i) + \".jpg\")\n",
    "        \n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T15:02:47.643789Z",
     "iopub.status.busy": "2024-04-28T15:02:47.643503Z",
     "iopub.status.idle": "2024-04-28T15:02:51.696274Z",
     "shell.execute_reply": "2024-04-28T15:02:51.695326Z",
     "shell.execute_reply.started": "2024-04-28T15:02:47.643765Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.make_archive(\"/kaggle/working/images\", 'zip', \"/kaggle/images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclussion\n",
    "In this work I trained a cycle GAN for Monet image generation, the overall project repository has two notebooks one with a local running and the other with an online Kaggle one. In future works it is possible to test other GPU architectures, batch sizes, and network architectures."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 1475600,
     "sourceId": 21755,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
